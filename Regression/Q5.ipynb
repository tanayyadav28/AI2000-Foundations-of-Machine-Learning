{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanay Yadav\n",
    "# AI20BTECH11026\n",
    "# Assignment 4 - Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training and testing arrays with the raw provided data\n",
    "\n",
    "X_train = np.array([[0.346, 0.780], [0.303, 0.439], [0.358, 0.729], [0.602, 0.863], [0.790, 0.753], [0.611, 0.965]])\n",
    "Y_train = np.array([[0], [0], [0], [1], [1], [1]])\n",
    "\n",
    "X_test = np.array([[0.959, 0.382], [0.750, 0.306], [0.395, 0.760], [0.823, 0.764], [0.761, 0.874], [0.844, 0.435]])\n",
    "Y_test = np.array([[0], [0], [0], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, weights):\n",
    "    '''\n",
    "    Parameters:\n",
    "    X : 1D array containing the datapoints\n",
    "    weights : Weights for the regressor model\n",
    "\n",
    "    Returns:\n",
    "    output of the input datapoints using the sigmoid function\n",
    "    '''\n",
    "    return 1/(1 + np.exp(-weights.T@X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(Y_hat, Y):\n",
    "    '''\n",
    "    Parameters:\n",
    "    Y_hat : 1D array containing the predicted outcomes from the regressor.\n",
    "    Y : actual outcomes for the given datapoints.\n",
    "\n",
    "    Returns:\n",
    "    Returns the cross entropy loss of the input predicted outcomes.\n",
    "    '''\n",
    "    return Y*np.log(Y_hat) + (1-Y)*np.log(1-Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor( ):    \n",
    "    def fit(X, Y, gamma, weights, epochs=10000):\n",
    "        '''\n",
    "        Parameters: \n",
    "        X: 2D array containing the training datapoints\n",
    "        Y: 1D array containing the outcomes to the input datapoints\n",
    "        weights: 1D array containing pre-intitialized weights\n",
    "        epochs: Number of iterations, default is 10000\n",
    "\n",
    "        Returns:\n",
    "        Total cross entropy loss over the number of epochs\n",
    "        '''\n",
    "        for i in range(epochs):\n",
    "            tot_loss = 0\n",
    "            for j in range(len(X)):\n",
    "                y_hat = sigmoid(X[j], weights)\n",
    "                weights -= gamma *X[j]*(y_hat - Y[j])/ len(X)\n",
    "                tot_loss += crossEntropyLoss(y_hat, int(Y[j]))\n",
    "            tot_loss = -(1/len(Y))*tot_loss\n",
    "        return tot_loss   \n",
    "    def predict(X, weights):\n",
    "        '''\n",
    "        Parameters:\n",
    "        X: 2D array containing the test datapoints\n",
    "        weights: 1D array containing the weights obtained from fitting the training set\n",
    "        \n",
    "        Returns:\n",
    "        y_hat: 1D array containing the predicted outcomes for the input datapoints\n",
    "        '''\n",
    "        y_hat = []\n",
    "        for i in range(len(X)):\n",
    "            y = sigmoid(X[i], weights)   \n",
    "            y_hat.append(1 if y>=0.5 else 0)\n",
    "        return y_hat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the training and testing datapoints\n",
    "\n",
    "temp = np.ones((6,1))\n",
    "X_train = np.hstack((temp, X_train))\n",
    "X_test = np.hstack((temp, X_test))\n",
    "weights = np.array([-1, 1.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Model $P(\\hat{y} = 1|x_1, x_2)$ is:  \n",
    "$\\ln\\left(\\frac{p}{1-p}\\right) = l = \\hat{y} = -1 + 1.5x_1 + 0.5x_2 = ln\\left(\\frac{P(\\hat{y}=1|x_1,x_2)}{1-P(\\hat{y}=1|x_1,x_2)}\\right)$    \n",
    "The cross entropy function is:  \n",
    "$E = y\\ln(\\hat{y}) + (1-y)\\ln(1 - \\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss after 1 iteration = 0.559503\n",
      "Weights = [-1.0027121   1.50560518  0.50232236]\n"
     ]
    }
   ],
   "source": [
    "# running the regressor for one iteration\n",
    "\n",
    "cel_1 = LogisticRegressor.fit(X_train, Y_train, 0.1, weights, 1)\n",
    "print('Cross Entropy Loss after 1 iteration = %.6f'%cel_1)\n",
    "print('Weights =', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Updated Logistic Model $P(\\hat{y} = 1|x_1, x_2)$ is:  \n",
    "$\\ln\\left(\\frac{p}{1-p}\\right) = l = \\hat{y} = -1.003 + 1.505x_1 + 0.502x_2 = ln\\left(\\frac{P(\\hat{y}=1|x_1,x_2)}{1-P(\\hat{y}=1|x_1,x_2)}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss after the convergence of gradient descent = 0.062421\n",
      "Weights = [-11.28397278  16.14187561   4.36162092]\n"
     ]
    }
   ],
   "source": [
    "# running the regressor until the gradient converges\n",
    "\n",
    "weights = np.array([-1, 1.5, 0.5])\n",
    "cel = LogisticRegressor.fit(X_train, Y_train, 0.1, weights)\n",
    "print('Cross Entropy Loss after the convergence of gradient descent = %.6f'%cel)\n",
    "print('Weights =', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Updated Logistic Model $P(\\hat{y} = 1|x_1, x_2)$ is:  \n",
    "$\\ln\\left(\\frac{p}{1-p}\\right) = l = \\hat{y} = -11.284 + 16.142x_1 + 4.361x_2 = ln\\left(\\frac{P(\\hat{y}=1|x_1,x_2)}{1-P(\\hat{y}=1|x_1,x_2)}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the outcomes over the test dataset.\n",
    "\n",
    "Y_predicted = LogisticRegressor.predict(X_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error : 0.3333\n",
      "Recall Score : 0.6\n",
      "Precision Score : 1.0\n"
     ]
    }
   ],
   "source": [
    "# obtaining the important metrics for the testing dataset.\n",
    "\n",
    "train_accuracy = accuracy_score(Y_predicted, Y_test)\n",
    "print('Validation Error : %.4f' %(1-train_accuracy))\n",
    "recall = recall_score(Y_predicted, Y_test)\n",
    "print('Recall Score :', recall)\n",
    "precision = precision_score(Y_predicted, Y_test)\n",
    "print('Precision Score :', precision)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
