{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: Tanay Yadav\n",
    "# Decision Tree\n",
    "# AI2000 - Foundations of Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "Fold: 1 / 10, accuracy: 0.8184\n",
      "Fold: 2 / 10, accuracy: 0.8408\n",
      "Fold: 3 / 10, accuracy: 0.7816\n",
      "Fold: 4 / 10, accuracy: 0.8265\n",
      "Fold: 5 / 10, accuracy: 0.8367\n",
      "Fold: 6 / 10, accuracy: 0.7939\n",
      "Fold: 7 / 10, accuracy: 0.8224\n",
      "Fold: 8 / 10, accuracy: 0.8102\n",
      "Fold: 9 / 10, accuracy: 0.7648\n",
      "Fold: 10 / 10, accuracy: 0.8282\n",
      "For Entropy based implementation, Average Accuracy = 0.8124\n"
     ]
    }
   ],
   "source": [
    "# Normal Implementation\n",
    "\n",
    "# importing required directories\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "my_name = \"Tanay-Madhav-Yadav\"\n",
    "\n",
    "\n",
    "# using entropy to select the best split\n",
    "\n",
    "def entropy(e_data):\n",
    "    wine_qual = len(e_data[0]) - 1            # index for the quality of wine (1/0)\n",
    "    num_0 = 0\n",
    "    num_1 = 0\n",
    "    for i in e_data:                          # calculating the instances of '1'\n",
    "        if i[wine_qual] == 1:\n",
    "            num_1 += 1\n",
    "        else:\n",
    "            num_0 += 1\n",
    "    p0 = num_0 / (num_0 + num_1)\n",
    "    p1 = 1 - p0\n",
    "    if p0 != 0:\n",
    "        p0 = -p0 * np.log2(p0)                 # calculation of entropy\n",
    "    if p1 != 0:\n",
    "        p1 = -p1 * np.log2(p1)\n",
    "    return p0 + p1\n",
    "\n",
    "\n",
    "# function to find the best possible split according to features and thresholds.\n",
    "\n",
    "def best_split(s_data):\n",
    "    best_split_dict = {'feature': None, 'threshold': None, 'left': [], 'right': []}    # dictionary to store the info\n",
    "    info_gain = -float('inf') + 1                                                      # about the best split\n",
    "    r_len = len(s_data[0]) - 1                # not considering the 'wine quality' column as feature\n",
    "    max_info_gain = -float('inf')\n",
    "\n",
    "    for feature in range(r_len):\n",
    "        values = [s_data[x][feature] for x in range(len(s_data))]\n",
    "        max_val = max(values)\n",
    "        min_val = min(values)\n",
    "        p_thresholds = np.linspace(min_val, max_val, 10)                     # creating abstract thresholds\n",
    "\n",
    "        for threshold in p_thresholds:\n",
    "            data_left = []                                                   # list to store the split data\n",
    "            data_right = []\n",
    "            for inputs in s_data:\n",
    "                if inputs[feature] < threshold:\n",
    "                    # dropping the feature utilised to split\n",
    "                    data_left.append([inputs[x] for x in [*range(0, feature), *range(feature + 1, r_len + 1)]])\n",
    "                else:\n",
    "                    data_right.append([inputs[x] for x in [*range(0, feature), *range(feature + 1, r_len + 1)]])\n",
    "\n",
    "            if len(data_left) > 0 and len(data_right) > 0:\n",
    "                # finding the best split by calculating information gain on each possible split.\n",
    "                info_gain = entropy(s_data) - ((len(data_left) / len(s_data)) * entropy(data_left) + (\n",
    "                        len(data_right) / len(s_data)) * entropy(data_right))\n",
    "            if info_gain > max_info_gain:\n",
    "                # storing the parameters for split into the dictionary\n",
    "                max_info_gain = info_gain\n",
    "                best_split_dict['feature'] = feature\n",
    "                best_split_dict['threshold'] = threshold\n",
    "                best_split_dict['left'] = data_left\n",
    "                best_split_dict['right'] = data_right\n",
    "    return best_split_dict\n",
    "\n",
    "\n",
    "# Implement your decision tree below\n",
    "class DecisionTree:\n",
    "    tree = {}\n",
    "\n",
    "    def learn(self, t_data):\n",
    "        # creating a tree using dictionary\n",
    "        tree = {'feature': None, 'value': None, 'threshold': None, 'left_tree': {}, 'right_tree': {}}\n",
    "        if len(t_data) > 0:\n",
    "            # checking if at least 2 features are available to differentiate and split and entropy is non-zero.\n",
    "            if len(t_data[0]) >= 2 and entropy(t_data) != 0:\n",
    "                b_split = best_split(t_data)\n",
    "                tree['feature'] = b_split['feature']\n",
    "                tree['threshold'] = b_split['threshold']\n",
    "\n",
    "                # recursively building the left and right sub-trees\n",
    "                left_tree = DecisionTree.learn(self, b_split['left'])\n",
    "                right_tree = DecisionTree.learn(self, b_split['right'])\n",
    "                tree['right_tree'] = right_tree\n",
    "                tree['left_tree'] = left_tree\n",
    "                return tree\n",
    "\n",
    "            # if there is just 1 feature remaining, it is a leaf\n",
    "            else:\n",
    "                tree['left_tree'] = None\n",
    "                tree['right_tree'] = None\n",
    "                label_idx = len(t_data[0]) - 1\n",
    "                label = [t_data[x][label_idx] for x in range(len(t_data))]\n",
    "                tree['value'] = max(label, key=label.count)\n",
    "                return tree\n",
    "\n",
    "    def classify(self, test_data, tree):\n",
    "        # ensuring that the tree has been trained\n",
    "        if tree:\n",
    "\n",
    "            # checking whether we have reached the leaf node\n",
    "            if tree['value'] is not None:\n",
    "                return tree['value']\n",
    "            feature_val = test_data[tree['feature']]\n",
    "\n",
    "            # classifying based on thresholds set during best_split()\n",
    "            if feature_val < tree['threshold']:\n",
    "                return self.classify(\n",
    "                    # leaving out already classified feature to increase accuracy.\n",
    "                    [test_data[x] for x in [*range(0, tree['feature']), *range(tree['feature'] + 1, len(test_data))]],\n",
    "                    tree['left_tree'])\n",
    "            else:\n",
    "                return self.classify(\n",
    "                    [test_data[x] for x in [*range(0, tree['feature']), *range(tree['feature'] + 1, len(test_data))]],\n",
    "                    tree['right_tree'])\n",
    "\n",
    "\n",
    "def run_decision_tree():\n",
    "    average_accu = 0\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = []\n",
    "        for line in csv.reader(f, delimiter=\",\"):\n",
    "            row = [float(x) for x in line]\n",
    "            data.append(row)\n",
    "    print(\"Number of records: %d\" % len(data))\n",
    "\n",
    "    # Split training/test sets\n",
    "    K = 10\n",
    "    for k in range(0, K):\n",
    "        training_set = [x for i, x in enumerate(data) if i % K != k]\n",
    "        test_set = [x for i, x in enumerate(data) if i % K == k]\n",
    "\n",
    "        tree = DecisionTree()\n",
    "        # Construct a tree using training set\n",
    "        Tree = tree.learn(training_set)\n",
    "\n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = tree.classify(instance[:-1], Tree)\n",
    "            results.append(result == instance[-1])\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True)) / float(len(results))\n",
    "        average_accu += accuracy\n",
    "        print('Fold:', k + 1, \"/ 10, accuracy: %.4f\" % accuracy)\n",
    "\n",
    "        # Writing results to a file (DO NOT CHANGE)\n",
    "        f = open(my_name + \"result.txt\", \"w\")\n",
    "        f.write(\"accuracy: %.4f\" % accuracy)\n",
    "        f.close()\n",
    "\n",
    "    print('For Entropy based implementation, Average Accuracy = %.4f' % (average_accu / K))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "Fold: 1 / 10, accuracy: 0.8082\n",
      "Fold: 2 / 10, accuracy: 0.8306\n",
      "Fold: 3 / 10, accuracy: 0.7837\n",
      "Fold: 4 / 10, accuracy: 0.8184\n",
      "Fold: 5 / 10, accuracy: 0.8245\n",
      "Fold: 6 / 10, accuracy: 0.8000\n",
      "Fold: 7 / 10, accuracy: 0.8000\n",
      "Fold: 8 / 10, accuracy: 0.8020\n",
      "Fold: 9 / 10, accuracy: 0.7751\n",
      "Fold: 10 / 10, accuracy: 0.8303\n",
      "For Gini-Index based implementation, Average Accuracy = 0.8073\n"
     ]
    }
   ],
   "source": [
    "# Gini based classification\n",
    "\n",
    "# importing required directories\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "my_name = \"Tanay-Madhav-Yadav\"\n",
    "\n",
    "\n",
    "# using gini to select the best split\n",
    "\n",
    "def gini(e_data):\n",
    "    wine_qual = len(e_data[0]) - 1            # index for the quality of wine (1/0)\n",
    "    num_0 = 0\n",
    "    num_1 = 0\n",
    "    for i in e_data:                          # calculating the instances of '1'\n",
    "        if i[wine_qual] == 1:\n",
    "            num_1 += 1\n",
    "        else:\n",
    "            num_0 += 1\n",
    "    p0 = num_0 / (num_0 + num_1)\n",
    "    p1 = 1 - p0\n",
    "    if p0 != 0:\n",
    "        p0 = p0 ** 2                 # calculation of gini index\n",
    "    if p1 != 0:\n",
    "        p1 = p1 ** 2\n",
    "    return 1 - p0 - p1\n",
    "\n",
    "\n",
    "# function to find the best possible split according to features and thresholds.\n",
    "\n",
    "def best_split(s_data):\n",
    "    best_split_dict = {'feature': None, 'threshold': None, 'left': [], 'right': []}    # dictionary to store the info\n",
    "    info_gain = -float('inf') + 1                                                      # about the best split\n",
    "    r_len = len(s_data[0]) - 1                # not considering the 'wine quality' column as feature\n",
    "    max_info_gain = -float('inf')\n",
    "\n",
    "    for feature in range(r_len):\n",
    "        values = [s_data[x][feature] for x in range(len(s_data))]\n",
    "        max_val = max(values)\n",
    "        min_val = min(values)\n",
    "        p_thresholds = np.linspace(min_val, max_val, 10)                     # creating abstract thresholds\n",
    "\n",
    "        for threshold in p_thresholds:\n",
    "            data_left = []                                                   # list to store the split data\n",
    "            data_right = []\n",
    "            for inputs in s_data:\n",
    "                if inputs[feature] < threshold:\n",
    "                    # dropping the feature utilised to split\n",
    "                    data_left.append([inputs[x] for x in [*range(0, feature), *range(feature + 1, r_len + 1)]])\n",
    "                else:\n",
    "                    data_right.append([inputs[x] for x in [*range(0, feature), *range(feature + 1, r_len + 1)]])\n",
    "\n",
    "            if len(data_left) > 0 and len(data_right) > 0:\n",
    "                # finding the best split by calculating information gain on each possible split.\n",
    "                info_gain = gini(s_data) - ((len(data_left) / len(s_data)) * gini(data_left) + (\n",
    "                        len(data_right) / len(s_data)) * gini(data_right))\n",
    "            if info_gain > max_info_gain:\n",
    "                # storing the parameters for split into the dictionary\n",
    "                max_info_gain = info_gain\n",
    "                best_split_dict['feature'] = feature\n",
    "                best_split_dict['threshold'] = threshold\n",
    "                best_split_dict['left'] = data_left\n",
    "                best_split_dict['right'] = data_right\n",
    "    return best_split_dict\n",
    "\n",
    "\n",
    "# Implement your decision tree below\n",
    "class DecisionTree:\n",
    "    tree = {}\n",
    "\n",
    "    def learn(self, t_data):\n",
    "        # creating a tree using dictionary\n",
    "        tree = {'feature': None, 'value': None, 'threshold': None, 'left_tree': {}, 'right_tree': {}}\n",
    "        if len(t_data) > 0:\n",
    "            # checking if at least 2 features are available to differentiate and split and gini enables split.\n",
    "            if len(t_data[0]) >= 2 and gini(t_data) > 0 and gini(t_data) < 0.5:\n",
    "                b_split = best_split(t_data)\n",
    "                tree['feature'] = b_split['feature']\n",
    "                tree['threshold'] = b_split['threshold']\n",
    "\n",
    "                # recursively building the left and right sub-trees\n",
    "                left_tree = DecisionTree.learn(self, b_split['left'])\n",
    "                right_tree = DecisionTree.learn(self, b_split['right'])\n",
    "                tree['right_tree'] = right_tree\n",
    "                tree['left_tree'] = left_tree\n",
    "                return tree\n",
    "\n",
    "            # if there is just 1 feature remaining, it is a leaf\n",
    "            else:\n",
    "                tree['left_tree'] = None\n",
    "                tree['right_tree'] = None\n",
    "                label_idx = len(t_data[0]) - 1\n",
    "                label = [t_data[x][label_idx] for x in range(len(t_data))]\n",
    "                tree['value'] = max(label, key=label.count)\n",
    "                return tree\n",
    "\n",
    "    def classify(self, test_data, tree):\n",
    "        # ensuring that the tree has been trained\n",
    "        if tree:\n",
    "\n",
    "            # checking whether we have reached the leaf node\n",
    "            if tree['value'] is not None:\n",
    "                return tree['value']\n",
    "            feature_val = test_data[tree['feature']]\n",
    "\n",
    "            # classifying based on thresholds set during best_split()\n",
    "            if feature_val < tree['threshold']:\n",
    "                return self.classify(\n",
    "                    # leaving out already classified feature to increase accuracy.\n",
    "                    [test_data[x] for x in [*range(0, tree['feature']), *range(tree['feature'] + 1, len(test_data))]],\n",
    "                    tree['left_tree'])\n",
    "            else:\n",
    "                return self.classify(\n",
    "                    [test_data[x] for x in [*range(0, tree['feature']), *range(tree['feature'] + 1, len(test_data))]],\n",
    "                    tree['right_tree'])\n",
    "\n",
    "\n",
    "def run_decision_tree():\n",
    "    average_accu = 0\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = []\n",
    "        for line in csv.reader(f, delimiter=\",\"):\n",
    "            row = [float(x) for x in line]\n",
    "            data.append(row)\n",
    "    print(\"Number of records: %d\" % len(data))\n",
    "\n",
    "    # Split training/test sets\n",
    "    K = 10\n",
    "    for k in range(0, K):\n",
    "        training_set = [x for i, x in enumerate(data) if i % K != k]\n",
    "        test_set = [x for i, x in enumerate(data) if i % K == k]\n",
    "\n",
    "        tree = DecisionTree()\n",
    "        # Construct a tree using training set\n",
    "        Tree = tree.learn(training_set)\n",
    "\n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = tree.classify(instance[:-1], Tree)\n",
    "            results.append(result == instance[-1])\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True)) / float(len(results))\n",
    "        average_accu += accuracy\n",
    "        print('Fold:', k + 1, \"/ 10, accuracy: %.4f\" % accuracy)\n",
    "\n",
    "        # Writing results to a file (DO NOT CHANGE)\n",
    "        f = open(my_name + \"result.txt\", \"w\")\n",
    "        f.write(\"accuracy: %.4f\" % accuracy)\n",
    "        f.close()\n",
    "\n",
    "    print('For Gini-Index based implementation, Average Accuracy = %.4f' % (average_accu / K))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Accuracy: 81.24\n",
    "Gini-Index Accuracy: 80.73\n",
    "\n",
    "The accuracy over Gini has decreased due to a few outliers, but gini-index is consistently accurate over the 10-folds of the cross validation performed in this code. This is because the Gini-index is specialised to handle univariate splits and information gain just calculates the entropy of the data before the split compared to the data after the split. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
